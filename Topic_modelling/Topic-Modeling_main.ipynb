{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING - TOP2VEC & GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation : séparation de l'ensemble du corpus et du sous-corpus d'urgence en 3 périodes (1789-1792, 1792-1795, 1795-1799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#pour l'approche top2vec : \n",
    "import json\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display\n",
    "import hdbscan\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "#pour représenter dans l'espaces les topic top2vec : \n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "#pour sauvegarder le wordcloud top2vec: \n",
    "import matplotlib.pyplot\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_decrets = pd.read_csv('sous_corpus/Ensemble_des_vols_concatenes.tsv', sep='\\t')\n",
    "decrets_urgence = pd.read_csv('sous_corpus/Ensemble_decrets_urgence.tsv', sep='\\t')\n",
    "\n",
    "ensemble_decrets = ensemble_decrets['texte'].dropna()\n",
    "decrets_urgence = decrets_urgence['texte'].dropna()\n",
    "\n",
    "ensemble_decrets_1789_1795 = ensemble_decrets[0:19877]\n",
    "decrets_urgence_1789_1795 = decrets_urgence[0:1102]\n",
    "\n",
    "decrets_non_urgents = ensemble_decrets[0:19877]\n",
    "mask = decrets_non_urgents.str.contains('urgence', case=False, na=False)\n",
    "decrets_non_urgent = decrets_non_urgents[mask== False]\n",
    "\n",
    "decrets_courrier_extraordinaire = ensemble_decrets[0:19877]\n",
    "mask = decrets_courrier_extraordinaire.str.contains(r'courrier.? extraordinaire.?', case=False, na=False)\n",
    "decrets_courrier_extraordinaire = ensemble_decrets_1789_1795[mask==True]\n",
    "\n",
    "decrets_bulletins = ensemble_decrets[0:19877]\n",
    "mask = decrets_bulletins.str.contains(r'au bulletin.?', case=False, na=False)\n",
    "decrets_bulletins = ensemble_decrets_1789_1795[mask==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Découpe des sous-corpus\n",
    "ensemble_decrets_1789_1792 = ensemble_decrets[0:5038]\n",
    "ensemble_decrets_1792_1795 = ensemble_decrets[5039:19877]\n",
    "decrets_urgence_1789_1792 = decrets_urgence[0:1101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_ensemble_decrets_1789_1795 = ensemble_decrets_1789_1795.tolist()\n",
    "liste_decrets_urgence_1789_1795 = decrets_urgence_1789_1795.tolist()\n",
    "liste_ensemble_decrets_urgence_avec_Directoire = decrets_urgence.tolist()\n",
    "liste_decrets_non_urgent = decrets_non_urgent.tolist()\n",
    "liste_decrets_courrier_extraordinaire = decrets_courrier_extraordinaire.tolist()\n",
    "liste_decrets_bulletins = decrets_bulletins.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sauvegarde des sous-corpus\n",
    "ensemble_decrets_1789_1795.to_csv('sous_corpus/ensemble_decrets_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_urgence_1789_1795.to_csv('sous_corpus/decrets_urgence_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_non_urgent.to_csv('sous_corpus/decrets_non_urgent.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_courrier_extraordinaire.to_csv('sous_corpus/decrets_courrier_extraordinaire.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_bulletins.to_csv('sous_corpus/decrets_bulletins.tsv', sep='\\t', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_decrets_1789_1795 = pd.read_csv('sous_corpus/ensemble_decrets_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_urgence_1789_1795 = pd.read_csv('sous_corpus/decrets_urgence_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_non_urgent = pd.read_csv('sous_corpus/decrets_non_urgent.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_courrier_extraordinaire =  pd.read_csv('sous_corpus/decrets_courrier_extraordinaire.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_bulletins = pd.read_csv('sous_corpus/decrets_bulletins.tsv', sep='\\t', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approches Top2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionnaires_topics(model, corpus_liste):\n",
    "    '''Fonction sortant tous les topics du corpus détectés par top2vec. Chaque topic comprend les différents mots\n",
    "    triés par poids, sous forme d'une liste de dictionnaire. \n",
    "    \n",
    "    corpus_liste : ensemble de textes au format liste. \n",
    "    '''\n",
    "    top2vec = model\n",
    "    nbre_topics = model.get_num_topics()\n",
    "    taille_topics = model.get_topic_sizes()\n",
    "\n",
    "    #pour avoir les topics en dictionnaire\n",
    "    topic_words, word_scores, topic_nums = top2vec.get_topics(nbre_topics)\n",
    "    liste_dicts = []\n",
    "    titres_plt = []\n",
    "    for numero_topic in topic_nums: \n",
    "        increment = 0\n",
    "        liste_tuples = []\n",
    "        sous_dictionnaire = {}\n",
    "\n",
    "        topic_size = taille_topics[0][numero_topic]\n",
    "        pourcentage_topic = topic_size/(len(corpus_liste))*100\n",
    "\n",
    "        for mot in topic_words[numero_topic]:\n",
    "            word_score_list = word_scores[numero_topic].tolist()\n",
    "\n",
    "            if increment in range(len(word_score_list)):\n",
    "                score = word_score_list[increment]\n",
    "                liste_mot = (mot, score)\n",
    "                liste_tuples.append(liste_mot)\n",
    "\n",
    "                increment += 1\n",
    "\n",
    "            else: increment=0\n",
    "\n",
    "        dict = {}\n",
    "        for a, b in liste_tuples:\n",
    "            dict.setdefault(a, b)\n",
    "        titres_plt.append(f\" le topic {numero_topic} représente {pourcentage_topic}  des documents; il est composé des mots:'\")\n",
    "        liste_dicts.append(dict)\n",
    "    return(liste_dicts)\n",
    "\n",
    "def titres_dico_topics(model, corpus_liste):\n",
    "    '''Fonction identique à la précédente, mais pour return le titre des dictionnaires (code redondant à reprendre) \n",
    "    '''\n",
    "    top2vec = model\n",
    "    nbre_topics = model.get_num_topics()\n",
    "    taille_topics = model.get_topic_sizes()\n",
    "\n",
    "    #pour avoir les topics en dictionnaire\n",
    "    topic_words, word_scores, topic_nums = top2vec.get_topics(nbre_topics)\n",
    "    liste_dicts = []\n",
    "    titres_plt = []\n",
    "    for numero_topic in topic_nums: \n",
    "        increment = 0\n",
    "        liste_tuples = []\n",
    "        sous_dictionnaire = {}\n",
    "\n",
    "        topic_size = taille_topics[0][numero_topic]\n",
    "        pourcentage_topic = topic_size/(len(corpus_liste))*100\n",
    "\n",
    "        for mot in topic_words[numero_topic]:\n",
    "            word_score_list = word_scores[numero_topic].tolist()\n",
    "\n",
    "            if increment in range(len(word_score_list)):\n",
    "                score = word_score_list[increment]\n",
    "                liste_mot = (mot, score)\n",
    "                liste_tuples.append(liste_mot)\n",
    "\n",
    "                increment += 1\n",
    "\n",
    "            else: increment=0\n",
    "\n",
    "        dict = {}\n",
    "        for a, b in liste_tuples:\n",
    "            dict.setdefault(a, b)\n",
    "        pourcentage_topic = round(pourcentage_topic, 2)\n",
    "        print(pourcentage_topic)\n",
    "        titres_plt.append(f\"Topic {numero_topic} ({pourcentage_topic} %  des documents)\")\n",
    "        liste_dicts.append(dict)\n",
    "    return(titres_plt)\n",
    "\n",
    "def spatialisation_topic(model, topic_reduction, path_plot_sortie):\n",
    "    ''' Fonction permettant de visualiser les topics dans l'espace. Ne return pas de variables mais enregistre les images.\n",
    "    \n",
    "    model : modèle top2vec entraîné; les données sont dans le modèle.\n",
    "    topic_reduction : forcément un nombre entier; c'est le nombre de topics qui sortiront de la réduction de topic\n",
    "                       nécessaire à l'affichage du plot. Ne peut être supérieur au nombres de topics totaux.\n",
    "    path_plot_sortie: chemin et nom du fichier à préciser, type path/file.jpg ; permet d'enregistrer la figure. \n",
    "    \n",
    "    '''\n",
    "    print(model.hierarchical_topic_reduction(topic_reduction))\n",
    "    #Présente les topics reliés entre eux dans la réduction du nombre de topics\n",
    "\n",
    "    umap_args = {\n",
    "        \"n_neighbors\": 15,\n",
    "        \"n_components\": 2, # 5 -> 2 for plotting \n",
    "        \"metric\": \"cosine\",\n",
    "    }\n",
    "\n",
    "    umap_model = umap.UMAP(**umap_args).fit(model._get_document_vectors(norm=False))\n",
    "    umap_plot = umap.plot.points(umap_model, labels=model.doc_top_reduced)\n",
    "    \n",
    "    umap_plot.figure.savefig(path_plot_sortie)\n",
    "    model.get_topics(topic_reduction,reduced=True)\n",
    "    \n",
    "def wordclouds(model, corpus_liste, savepath, background_color=\"white\", colormap=\"matplotlib.cm.ocean\"):\n",
    "    '''Fonction générant un nuage de mot suivant le dictionnaire des topics proposés. \n",
    "    Enregistre également ces nuages dans des dossiers spécifiés.\n",
    "    model : modèle top2vec\n",
    "    corpus_liste : liste des textes du corpus (pour calcul du poids du topic)\n",
    "    savepath : dossier dans lequel sauvegarder le fichier\n",
    "    colormap : à choisir parmi les colormap de matplotlib.cm'''\n",
    "    \n",
    "    dictionnaires = dictionnaires_topics(model,corpus_liste)\n",
    "    titre = titres_dico_topics(model, corpus_liste)\n",
    "    \n",
    "    count=0\n",
    "    for dico in dictionnaires:\n",
    "        plt.figure(figsize=(16,4), dpi=200)\n",
    "        wordcloud = WordCloud(background_color=background_color, width=1600, height=400, color_func=lambda *args, **kwargs: (0,0,0), colormap=colormap).generate_from_frequencies(dico)\n",
    "        plt.figure()\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{titre[count]}\")\n",
    "        plt.savefig(f'{savepath}/topic_{count}.png')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Par période (plus précis mais moins de données en entrée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_ensemble_decrets_1789_1795 = ensemble_decrets_1789_1795.tolist()\n",
    "liste_decrets_urgence_1789_1795 = decrets_urgence_1789_1795.tolist()\n",
    "liste_ensemble_decrets_urgence_avec_Directoire = decrets_urgence.tolist()\n",
    "liste_decrets_non_urgent = decrets_non_urgent.tolist()\n",
    "liste_decrets_courrier_extraordinaire = decrets_courrier_extraordinaire.tolist()\n",
    "liste_decrets_bulletins = decrets_bulletins.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_ensemble_decrets_1789_1795 = Top2Vec(documents=liste_ensemble_decrets_1789_1795, speed=\"learn\", workers=4)\n",
    "top2vec_ensemble_decrets_1789_1795.save(\"modeles/top2vec_ensemble_decrets_1789_1795\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_decrets_urgence_1789_1795 = Top2Vec(documents=liste_decrets_urgence_1789_1795, speed=\"learn\", workers=4)\n",
    "top2vec_decrets_urgence_1789_1795.save(\"modeles/top2vec_decrets_urgence_1789_1795\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_decrets_urgence_1789_1795_deeplearn = Top2Vec(documents=liste_decrets_urgence_1789_1795, speed=\"deep-learn\", workers=4)\n",
    "top2vec_decrets_urgence_1789_1795_deeplearn.save(\"modeles/top2vec_decrets_urgence_1789_1795_deeplearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_liste_decrets_non_urgent = Top2Vec(documents=liste_decrets_non_urgent, speed=\"learn\", workers=4)\n",
    "top2vec_liste_decrets_non_urgent.save(\"modeles/top2vec_liste_decrets_non_urgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_liste_decrets_courrier_extraordinaire = Top2Vec(documents=liste_decrets_courrier_extraordinaire, speed=\"learn\", workers=4)\n",
    "top2vec_liste_decrets_courrier_extraordinaire.save(\"modeles/top2vec_liste_decrets_courrier_extraordinaire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_decrets_bulletins = Top2Vec(documents=liste_decrets_bulletins, speed=\"deep-learn\", workers=4)\n",
    "top2vec_decrets_bulletins.save(\"modeles/top2vec_decrets_bulletins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARGEMENT ET ANALYSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_decrets_1789_1795 = pd.read_csv('sous_corpus/ensemble_decrets_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_urgence_1789_1795 = pd.read_csv('sous_corpus/decrets_urgence_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_non_urgent = pd.read_csv('sous_corpus/decrets_non_urgent.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_bulletins = pd.read_csv('sous_corpus/decrets_bulletins.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "top2vec_decrets_bulletins = Top2Vec.load(\"modeles/top2vec_decrets_bulletins\")\n",
    "top2vec_liste_decrets_non_urgent = Top2Vec.load(\"modeles/top2vec_liste_decrets_non_urgent\")\n",
    "top2vec_decrets_urgence_1789_1795 = Top2Vec.load(\"modeles/top2vec_decrets_urgence_1789_1795\")\n",
    "top2vec_decrets_urgence_1789_1795_deeplearn = Top2Vec.load(\"modeles/top2vec_decrets_urgence_1789_1795_deeplearn\")\n",
    "top2vec_ensemble_decrets_1789_1795 = Top2Vec.load(\"modeles/top2vec_ensemble_decrets_1789_1795\")\n",
    "top2vec_ensemble_decrets_urgence = Top2Vec.load(\"modeles_Top2Vec/Top2Vec_learn_4workers_ensemble_decrets_urgence\")\n",
    "\n",
    "liste_ensemble_decrets_1789_1795 = ensemble_decrets_1789_1795.tolist()\n",
    "liste_decrets_urgence_1789_1795 = decrets_urgence_1789_1795.tolist()\n",
    "liste_ensemble_decrets_urgence_avec_Directoire = decrets_urgence.tolist()\n",
    "liste_decrets_non_urgent = decrets_non_urgent.tolist()\n",
    "liste_decrets_bulletins = decrets_bulletins.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation des wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordclouds(top2vec_decrets_bulletins, liste_decrets_bulletins, 'WORDCLOUDS/inscription_bulletin_deeplearn', background_color=\"white\", colormap=\"copper\")\n",
    "wordclouds(top2vec_liste_decrets_non_urgent, liste_decrets_non_urgent, 'WORDCLOUDS/non_urgents', background_color=\"white\", colormap=\"copper\")\n",
    "wordclouds(top2vec_ensemble_decrets_urgence, liste_ensemble_decrets_urgence_avec_Directoire, 'WORDCLOUDS/urgence_avec_directoire', background_color=\"white\", colormap=\"copper\")\n",
    "wordclouds(top2vec_decrets_urgence_1789_1795, liste_decrets_urgence_1789_1795, 'WORDCLOUDS/urgence_1789_1795', background_color=\"white\", colormap=\"copper\")\n",
    "wordclouds(top2vec_decrets_urgence_1789_1795_deeplearn, liste_decrets_urgence_1789_1795, 'WORDCLOUDS/urgence_1789_1795_deeplearn', background_color=\"white\", colormap=\"copper\")\n",
    "wordclouds(top2vec_ensemble_decrets_1789_1795, liste_ensemble_decrets_1789_1795, 'WORDCLOUDS/ensemble_decrets_1789_1795', background_color=\"white\", colormap=\"copper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top2vec_decrets_bulletins.get_num_topics(),\n",
    "top2vec_liste_decrets_courrier_extraordinaire.get_num_topics(),\n",
    "top2vec_liste_decrets_non_urgent.get_num_topics(),\n",
    "top2vec_decrets_urgence_1789_1795.get_num_topics(),\n",
    "top2vec_ensemble_decrets_1789_1795.get_num_topics(), \n",
    "top2vec_ensemble_decrets_urgence.get_num_topics(),\n",
    "top2vec_decrets_urgence_1789_1795_deeplearn.get_num_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataall = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#FF0000', '#FF8000', '#00FF00', '#00FFFF','#0000FF','#8000FF','#FF00FF','#0080FF','#FFFF00']}\n",
    "dfall = pd.DataFrame(dataall)\n",
    "\n",
    "data0 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#FF0000', '#CECECE', '#CECECE', '#CECECE','#CECECE','#CECECE','#CECECE','#CECECE','#CECECE']}\n",
    "df0 = pd.DataFrame(data0)\n",
    "\n",
    "data1 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#FF8000', '#CECECE', '#CECECE','#CECECE','#CECECE','#CECECE','#CECECE','#CECECE']}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "data2 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#CECECE', '#00FF00', '#CECECE','#CECECE','#CECECE','#CECECE','#CECECE','#CECECE']}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "data3 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#CECECE', '#CECECE', '#00FFFF','#CECECE','#CECECE','#CECECE','#CECECE','#CECECE']}\n",
    "df3 = pd.DataFrame(data3)\n",
    "\n",
    "data4 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#CECECE', '#CECECE', '#CECECE','#0000FF','#CECECE','#CECECE','#CECECE','#CECECE']}\n",
    "df4 = pd.DataFrame(data4)\n",
    "\n",
    "data5 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#CECECE', '#CECECE', '#CECECE','#CECECE','#8000FF','#CECECE','#CECECE','#CECECE']}\n",
    "df5 = pd.DataFrame(data5)\n",
    "\n",
    "data6 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#CECECE', '#CECECE', '#CECECE','#CECECE','#CECECE','#FF00FF','#CECECE','#CECECE']}\n",
    "df6 = pd.DataFrame(data6)\n",
    "\n",
    "data7 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#CECECE', '#CECECE', '#CECECE','#CECECE','#CECECE','#CECECE','#0080FF','#CECECE']}\n",
    "df7 = pd.DataFrame(data7)\n",
    "\n",
    "data8 = {'doc_top': ['0', '1', '2', '3', '4', '5', '6', '7', '8'], 'color': ['#CECECE', '#CECECE', '#CECECE', '#CECECE','#CECECE','#CECECE','#CECECE','#CECECE','#FFFF00']}\n",
    "df8 = pd.DataFrame(data8)\n",
    "\n",
    "umap_args_model = {\n",
    "\"n_neighbors\": 15,\n",
    "\"n_components\": 2,\n",
    "\"metric\": \"cosine\",\n",
    "'min_dist':0,\n",
    "}\n",
    "umap_model = umap.UMAP(**umap_args_model).fit(top2vec_decrets_urgence_1789_1795_deeplearn._get_document_vectors(norm=False))\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = dfall['color'], background='white')\n",
    "print(\"\"\"All the topics \"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df0['color'], background='white')\n",
    "print(\"\"\"Projection du topic 0 :'patrie', 'empire', 'defense', 'armes', 'citoyens', 'frontieres',\n",
    "         'requisitions', 'pouvoir', 'zele', 'executif', 'ennemis',\n",
    "         'requisition', 'liberte', 'peuple', 'publique', 'moyens',\n",
    "         'mesures', 'surete', 'importe', 'ennemi', 'seroit', 'force',\n",
    "         'formation', 'prompte', 'colonies', 'armement', 'champ', 'salut',\n",
    "         'tous', 'tranquillite', 'promptement', 'servir', 'sections',\n",
    "         'gardes', 'bataillons', 'se', 'plus', 'nationales', 'sont',\n",
    "         'francaise', 'constitution', 'representans', 'administratifs',\n",
    "         'considerant', 'territoire', 'convenable', 'nation', 'poste',\n",
    "         'volontaires', 'faire' \"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df1['color'], background='white')\n",
    "print(\"\"\"Projection du topic 1 :'fonds', 'recettes', 'depenses', 'secours', 'extraordinaires',\n",
    "         'somme', 'tresorerie', 'disposition', 'interieur', 'millions',\n",
    "         'extraordinaire', 'finances', 'avance', 'comites', 'caisse',\n",
    "         'avances', 'tiendra', 'travaux', 'ordinaire', 'mille',\n",
    "         'accordes', 'besoins', 'livres', 'montant', 'publics', 'urgence',\n",
    "         'accorder', 'decrete', 'payeurs', 'liv', 'sieurs', 'enfans',\n",
    "         'tresor', 'interet', 'ministre', 'nationale', 'vu', 'rapport',\n",
    "         'considerant', 'public', 'due', 'sieur', 'mois', 'hopitaux',\n",
    "         'entendu', 'derniers', 'proportion', 'liquidation', 'mis',\n",
    "         'compte'\"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df2['color'], background='white')\n",
    "print(\"\"\"Projection du topic 2 :'gendarmerie', 'gendarmes', 'rang', 'anciennete', 'grade',\n",
    "         'lieutenans', 'grades', 'lieutenant', 'logis', 'vaisseaux',\n",
    "         'places', 'marechal', 'colonel', 'capitaines', 'choix',\n",
    "         'organisation', 'compagnies', 'officiers', 'divisions',\n",
    "         'service', 'services', 'militaire', 'activite', 'marine',\n",
    "         'colonels', 'jouiront', 'adjudans', 'supplement', 'appointemens',\n",
    "         'parmi', 'division', 'major', 'capitaine', 'formation', 'age',\n",
    "         'nomination', 'servi', 'artillerie', 'remplir', 'commissions',\n",
    "         'augmentation', 'regimens', 'camp', 'pied', 'campagne',\n",
    "         'troupes', 'infanterie', 'moitie', 'traitement', 'ans'\"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df3['color'], background='white')\n",
    "print(\"\"\"Projection du topic 3 :'juges', 'tribunal', 'jures', 'tribunaux', 'fonctions',\n",
    "         'remplir', 'procureur', 'election', 'membres', 'syndic',\n",
    "         'nomination', 'assemblees', 'jugement', 'proceder', 'commune',\n",
    "         'haute', 'exercice', 'paix', 'jugemens', 'sections', 'defaut',\n",
    "         'cour', 'aupres', 'president', 'arrondissement', 'tranquillite',\n",
    "         'commis', 'section', 'juge', 'nommes', 'seul', 'ville', 'loi',\n",
    "         'epoux', 'pourvoir', 'directoire', 'conseil', 'instruction',\n",
    "         'heures', 'sauf', 'paris', 'sans', 'district', 'nomme', 'police',\n",
    "         'commissaires', 'expedition', 'parmi', 'conduite', 'liste'\"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df4['color'], background='white')\n",
    "print(\"\"\"Projection du topic 4 :'rentes', 'creanciers', 'liquidation', 'biens', 'payeurs',\n",
    "         'contrats', 'emigres', 'titres', 'enregistrement', 'paiement',\n",
    "         'dues', 'quittances', 'edit', 'certificats', 'domaines',\n",
    "         'remboursement', 'rente', 'receveurs', 'certificat', 'femme',\n",
    "         'etats', 'formalites', 'supprimes', 'montant', 'pensions',\n",
    "         'epoux', 'contrat', 'profit', 'acte', 'caisse', 'bureaux',\n",
    "         'obtenir', 'mains', 'residence', 'reconnoissance',\n",
    "         'extraordinaire', 'francais', 'tenus', 'presenter', 'etoient',\n",
    "         'jugemens', 'prescrite', 'agent', 'marie', 'derniers',\n",
    "         'recettes', 'personne', 'regie', 'inscrite', 'interet'\"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df5['color'], background='white')\n",
    "print(\"\"\"Projection du topic 5 :'vu', 'adjudication', 'loire', 'departement', 'commune', 'avis',\n",
    "         'etablissement', 'ville', 'directoire', 'district', 'saint',\n",
    "         'proceder', 'domaines', 'maison', 'envoye', 'finances',\n",
    "         'administrateurs', 'administration', 'oui', 'rapport', 'bureaux',\n",
    "         'haute', 'demande', 'marche', 'tribunal', 'administratifs',\n",
    "         'regie', 'assemblees', 'definitivement', 'necessite', 'acte',\n",
    "         'due', 'biens', 'autorise', 'arrete', 'depense', 'petition',\n",
    "         'montant', 'municipalite', 'interieur', 'urgence', 'ladite',\n",
    "         'comite', 'vente', 'arretes', 'apres', 'contributions', 'avoir',\n",
    "         'son', 'etablir'\"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df6['color'], background='white')\n",
    "print(\"\"\"Projection du topic 6 :'monnoies', 'especes', 'publiques', 'fabrication',\n",
    "         'contributions', 'hotel', 'commission', 'poids', 'etablissemens',\n",
    "         'directeurs', 'matieres', 'administrations', 'argent',\n",
    "         'surveillance', 'pieces', 'quantite', 'commissaires', 'comptes',\n",
    "         'assignats', 'etablissement', 'regie', 'employer', 'receveurs',\n",
    "         'circulation', 'travaux', 'maisons', 'remis', 'operations',\n",
    "         'enregistrement', 'objets', 'convenable', 'demeure', 'vivres',\n",
    "         'concerne', 'bureau', 'examen', 'distribution', 'responsabilite',\n",
    "         'etats', 'passer', 'etablis', 'directeur', 'arrondissement',\n",
    "         'supprimes', 'particuliers', 'travail', 'peuvent', 'armees',\n",
    "         'art', 'remettre'\"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df7['color'], background='white')\n",
    "print(\"\"\"Projection du topic 7 :'assignats', 'fabrication', 'circulation', 'papier', 'millions',\n",
    "         'extraordinaire', 'caisse', 'monnoies', 'archives', 'valeur',\n",
    "         'dessous', 'publiques', 'directeurs', 'caisses', 'papiers',\n",
    "         'contributions', 'procede', 'receveurs', 'tresorerie',\n",
    "         'cinquante', 'directeur', 'marches', 'quantite', 'finances',\n",
    "         'tresorier', 'districts', 'recettes', 'livres', 'mesure',\n",
    "         'administrateur', 'operations', 'cinq', 'cent', 'remis',\n",
    "         'responsabilite', 'domaines', 'decrets', 'dix', 'livre',\n",
    "         'sections', 'hotel', 'assemblee', 'commissaires', 'sols',\n",
    "         'etablissement', 'somme', 'an', 'ordonne', 'remettre', 'quinze'\"\"\")\n",
    "umap.plot.show(model2)\n",
    "\n",
    "model2 = umap.plot.points(umap_model, labels = top2vec_decrets_urgence_1789_1795_deeplearn.doc_top, color_key = df8['color'], background='white')\n",
    "print(\"\"\"Projection du topic 8 :'commune', 'interets', 'vu', 'avis', 'ladite', 'employee',\n",
    "         'vente', 'remboursement', 'directoire', 'adjudication',\n",
    "         'contributions', 'administratifs', 'district', 'ville',\n",
    "         'necessite', 'departement', 'loire', 'montant', 'oui',\n",
    "         'employer', 'territoire', 'charge', 'grains', 'municipalite',\n",
    "         'ses', 'biens', 'pourvoir', 'somme', 'subsistances', 'paiement',\n",
    "         'ordinaire', 'extraordinaire', 'finances', 'demande', 'acte',\n",
    "         'annees', 'regie', 'pourroit', 'domaines', 'besoins', 'procurer',\n",
    "         'subsistance', 'surveillance', 'sols', 'definitivement',\n",
    "         'autorise', 'interieur', 'faire', 'etablis', 'caisse'\"\"\")\n",
    "umap.plot.show(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELLING - GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import tqdm.notebook as tqdm\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=[\"NOUN\"], stopwords = ['décréter','citoyen','assemblee', 'decret', 'urgence', 'que', 'ne', 'il', 'ci', 'loi', 'teneur', 'ce', 'celle', 'assemblée', 'considérer', 'approuve', 'acte', 'conseil', 'adopter', 'signer', 'preceder', 'suivant', 'suivre','au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX', 'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXVI', 'XXVII', 'XXVIII', 'XXIX', 'XXX', 'XXXI', 'XXXII', 'XXXIII', 'XXXIV', 'XXXV', 'XXXVI', 'XXXVII', 'XXXVIII', 'XXXIX', 'XL', 'XLI', 'XLII', 'XLIII', 'XLIV', 'XLV', 'XLVI', 'XLVII', 'XLVIII', 'XLIX', 'L', 'LI', 'LII', 'LIII', 'LIV', 'LV', 'LVI', 'LVII', 'LVIII', 'LIX', 'LX', 'LXI', 'LXII', 'LXIII', 'LXIV', 'LXV', 'LXVI', 'LXVII', 'LXVIII', 'LXIX', 'LXX', 'LXXI', 'LXXII', 'LXXIII', 'LXXIV', 'LXXV', 'LXXVI', 'LXXVII', 'LXXVIII', 'LXXIX', 'LXXX', 'LXXXI', 'LXXXII', 'LXXXIII', 'LXXXIV', 'LXXXV', 'LXXXVI', 'LXXXVII', 'LXXXVIII', 'LXXXIX', 'XC', 'XCI', 'XCII', 'XCIII', 'XCIV', 'XCV', 'XCVI', 'XCVII', 'XCVIII', 'XCIX', 'C']):\n",
    "    '''Fonction lemmatisant un texte en vue d'une approche LDA. Permet de cleaner le texte également en utilisant SpaCy. \n",
    "    \n",
    "    texts : textes à lemmatiser (format df['texte']). \n",
    "    allowed_postags: POS qui seront conservés (détectés par Spacy). \n",
    "    stopwords: listes de mots qui ne seront pas considérés et supprimés du corpus lemmatisé. \n",
    "    \n",
    "    '''\n",
    "    nlp = spacy.load(\"fr_core_news_md\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags and token.lemma_ not in stopwords:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)\n",
    "\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "def id2word_construction(data_words):\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    corpus = []\n",
    "    for text in data_words:\n",
    "        new = id2word.doc2bow(text)\n",
    "        corpus.append(new)\n",
    "    return(corpus)\n",
    "\n",
    "def lda_model_construction(corpus, id2word, nbre_topics=int):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=nbre_topics, #à faire correspondre avec le nbre trouvé par Top2Vec,\n",
    "                                               update_every=1,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha=\"auto\")\n",
    "    return(lda_model)\n",
    "\n",
    "stopwords = ['décréter', 'citoyen', 'assemblee', 'decret', 'urgence', 'que', 'ne', 'il', 'ci', 'loi', 'teneur', 'ce', 'celle', 'assemblée', 'considérer', 'approuve', 'acte', 'conseil', 'adopter', 'signer', 'preceder', 'suivant', 'suivre','au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX', 'X', 'XI', 'XII', 'XIII', 'XIV', 'XV', 'XVI', 'XVII', 'XVIII', 'XIX', 'XX', 'XXI', 'XXII', 'XXIII', 'XXIV', 'XXV', 'XXVI', 'XXVII', 'XXVIII', 'XXIX', 'XXX', 'XXXI', 'XXXII', 'XXXIII', 'XXXIV', 'XXXV', 'XXXVI', 'XXXVII', 'XXXVIII', 'XXXIX', 'XL', 'XLI', 'XLII', 'XLIII', 'XLIV', 'XLV', 'XLVI', 'XLVII', 'XLVIII', 'XLIX', 'L', 'LI', 'LII', 'LIII', 'LIV', 'LV', 'LVI', 'LVII', 'LVIII', 'LIX', 'LX', 'LXI', 'LXII', 'LXIII', 'LXIV', 'LXV', 'LXVI', 'LXVII', 'LXVIII', 'LXIX', 'LXX', 'LXXI', 'LXXII', 'LXXIII', 'LXXIV', 'LXXV', 'LXXVI', 'LXXVII', 'LXXVIII', 'LXXIX', 'LXXX', 'LXXXI', 'LXXXII', 'LXXXIII', 'LXXXIV', 'LXXXV', 'LXXXVI', 'LXXXVII', 'LXXXVIII', 'LXXXIX', 'XC', 'XCI', 'XCII', 'XCIII', 'XCIV', 'XCV', 'XCVI', 'XCVII', 'XCVIII', 'XCIX', 'C']\n",
    "\n",
    "def topic_modelling_LDA(data, nombre_topics, path_model):\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "    data_words = gen_words(lemmatized_texts)\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    corpus = id2word_construction(data_words)\n",
    "    lda_model = lda_model_construction(corpus, id2word, nombre_topics)\n",
    "    return(lda_model)\n",
    "\n",
    "def nmf_model_construction(corpus, id2word, nbre_topics=int):\n",
    "    nmf_model = gensim.models.nmf.Nmf(corpus=corpus,\n",
    "                                     id2word=id2word,\n",
    "                                     num_topics=nbre_topics,\n",
    "                                     random_state=100,\n",
    "                                     chunksize=100)\n",
    "    return(nmf_model)\n",
    "\n",
    "def topic_modelling_NMF(data, nombre_topics, path_model):\n",
    "    lemmatized_texts = lemmatization(data)\n",
    "    data_words = gen_words(lemmatized_texts)\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    corpus = id2word_construction(data_words)\n",
    "    nmf_model = nmf_model_construction(corpus, id2word, nombre_topics)\n",
    "    return(nmf_model)\n",
    "    \n",
    "\n",
    "def wordcloud_gensim(model, savepath, number_of_words, nombre_topics=int):\n",
    "    for index, topic in model.show_topics(num_topics = nombre_topics,formatted=False, num_words=number_of_words):\n",
    "        wordcloud_to_be = [w[0] for w in topic]\n",
    "        plt.figure(figsize=(16,4), dpi=200)\n",
    "        unique_string=(\" \").join(wordcloud_to_be)\n",
    "        wordcloud = WordCloud(background_color=\"white\", width=1600, height=400, color_func=lambda *args, **kwargs: (0,0,0)).generate(unique_string)\n",
    "        plt.figure()\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"decrets_bulletins_topic_{index}\")\n",
    "        plt.savefig(f'{savepath}/topic_{index}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initalisation des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_decrets_1789_1795 = pd.read_csv('sous_corpus/ensemble_decrets_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_urgence_1789_1795 = pd.read_csv('sous_corpus/decrets_urgence_1789_1795.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_non_urgent = pd.read_csv('sous_corpus/decrets_non_urgent.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "decrets_bulletins = pd.read_csv('sous_corpus/decrets_bulletins.tsv', sep='\\t', encoding=\"utf-8\")\n",
    "ensemble_decrets = pd.read_csv('sous_corpus/Ensemble_des_vols_concatenes.tsv', sep='\\t')\n",
    "decrets_urgence = pd.read_csv('sous_corpus/Ensemble_decrets_urgence.tsv', sep='\\t')\n",
    "ensemble_decrets = ensemble_decrets['texte'].dropna()\n",
    "decrets_urgence = decrets_urgence['texte'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement des modèles Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_bulletins = topic_modelling_LDA(decrets_bulletins['texte'], 4, 'NEW_GENSIM_MODEL/decrets_bulletins')\n",
    "wordcloud_gensim(LDA_decrets_bulletins, 'NEW_GENSIM_WORDCLOUDS/LDA/inscription_bulletin_deeplearn', 25)\n",
    "NMF_decrets_bulletins = topic_modelling_NMF(decrets_bulletins['texte'], 4, 'NEW_GENSIM_MODEL/decrets_bulletins')\n",
    "wordcloud_gensim(NMF_decrets_bulletins, 'NEW_GENSIM_WORDCLOUDS/NMF/inscription_bulletin_deeplearn', 25)\n",
    "LDA_decrets_bulletins.save('NEW_GENSIM_MODELS/LDA/inscription_bulletin')\n",
    "NMF_decrets_bulletins.save('NEW_GENSIM_MODELS/NMF/inscription_bulletin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_non_urgent = topic_modelling_LDA(decrets_non_urgent['texte'], 157, 'NEW_GENSIM_MODEL/non_urgent')\n",
    "LDA_decrets_non_urgent.save('NEW_GENSIM_MODELS/LDA/decrets_non_urgents')\n",
    "NMF_decrets_non_urgent = topic_modelling_NMF(decrets_non_urgent['texte'], 157, 'NEW_GENSIM_MODEL/non_urgent')\n",
    "NMF_decrets_non_urgent.save('NEW_GENSIM_MODELS/NMF/decrets_non_urgents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_decrets_non_urgent.save('NEW_GENSIM_MODELS/NMF/decrets_non_urgents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_non_urgent = models.LdaModel.load('NEW_GENSIM_MODELS/LDA/decrets_non_urgents')\n",
    "NMF_decrets_non_urgent = models.Nmf.load('NEW_GENSIM_MODELS/NMF/decrets_non_urgents')\n",
    "wordcloud_gensim(LDA_decrets_non_urgent, 'NEW_GENSIM_WORDCLOUDS/LDA/decrets_non_urgents', 25, 157)\n",
    "wordcloud_gensim(NMF_decrets_non_urgent, 'NEW_GENSIM_WORDCLOUDS/NMF/decrets_non_urgents', 25, 157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_urgence_1789_1795 = topic_modelling_LDA(decrets_urgence_1789_1795['texte'], 10, 'NEW_GENSIM_MODEL/decrets_urgence_1789_1795')\n",
    "NMF_decrets_urgence_1789_1795 = topic_modelling_NMF(decrets_urgence_1789_1795['texte'], 10, 'NEW_GENSIM_MODEL/decrets_urgence_1789_1795')\n",
    "LDA_decrets_urgence_1789_1795.save('NEW_GENSIM_MODELS/LDA/decrets_urgence_1789_1795')\n",
    "NMF_decrets_urgence_1789_1795.save('NEW_GENSIM_MODELS/NMF/decrets_urgence_1789_1795')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_urgence_1789_1795 = models.LdaModel.load('NEW_GENSIM_MODELS/LDA/decrets_urgence_1789_1795')\n",
    "NMF_decrets_urgence_1789_1795 = models.Nmf.load('NEW_GENSIM_MODELS/NMF/decrets_urgence_1789_1795')\n",
    "wordcloud_gensim(NMF_decrets_urgence_1789_1795, 'NEW_GENSIM_WORDCLOUDS/NMF/decrets_urgence_1789_1795', 25, 10)\n",
    "wordcloud_gensim(LDA_decrets_urgence_1789_1795, 'NEW_GENSIM_WORDCLOUDS/LDA/decrets_urgence_1789_1795', 25, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_ensemble_decrets_1789_1795 = topic_modelling_LDA(ensemble_decrets_1789_1795['texte'], 165, 'NEW_GENSIM_MODEL/ensemble_decrets_1789_1795')\n",
    "NMF_ensemble_decrets_1789_1795 = topic_modelling_NMF(ensemble_decrets_1789_1795['texte'], 165, 'NEW_GENSIM_MODEL/ensemble_decrets_1789_1795')\n",
    "LDA_ensemble_decrets_1789_1795.save('NEW_GENSIM_MODELS/LDA/ensemble_decrets_1789_1795')\n",
    "NMF_ensemble_decrets_1789_1795.save('NEW_GENSIM_MODELS/NMF/ensemble_decrets_1789_1795')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_ensemble_decrets_1789_1795 = models.LdaModel.load('NEW_GENSIM_MODELS/LDA/ensemble_decrets_1789_1795')\n",
    "NMF_ensemble_decrets_1789_1795 = models.Nmf.load('NEW_GENSIM_MODELS/NMF/ensemble_decrets_1789_1795')\n",
    "wordcloud_gensim(NMF_ensemble_decrets_1789_1795, 'NEW_GENSIM_WORDCLOUDS/NMF/ensemble_decrets_1789_1795', 25, 165)\n",
    "wordcloud_gensim(LDA_ensemble_decrets_1789_1795, 'NEW_GENSIM_WORDCLOUDS/LDA/ensemble_decrets_1789_1795', 25, 165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_urgence = topic_modelling_LDA(decrets_urgence, 40, 'NEW_GENSIM_MODEL/decrets_urgence')\n",
    "NMF_decrets_urgence = topic_modelling_NMF(decrets_urgence, 40, 'NEW_GENSIM_MODEL/decrets_urgence')\n",
    "LDA_decrets_urgence.save('NEW_GENSIM_MODELS/LDA/decrets_urgence')\n",
    "NMF_decrets_urgence.save('NEW_GENSIM_MODELS/NMF/decrets_urgence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_urgence = models.LdaModel.load('NEW_GENSIM_MODELS/LDA/decrets_urgence')\n",
    "NMF_decrets_urgence = models.Nmf.load('NEW_GENSIM_MODELS/NMF/decrets_urgence')\n",
    "wordcloud_gensim(NMF_decrets_urgence, 'NEW_GENSIM_WORDCLOUDS/NMF/decrets_urgence', 25, 40)\n",
    "wordcloud_gensim(LDA_decrets_urgence, 'NEW_GENSIM_WORDCLOUDS/LDA/decrets_urgence', 25, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_urgence2 = topic_modelling_LDA(decrets_urgence, 40, 'NEW_GENSIM_MODEL/decrets_urgence_2')\n",
    "LDA_decrets_urgence2.save('NEW_GENSIM_MODELS/LDA/decrets_urgence_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_decrets_urgence = models.LdaModel.load('NEW_GENSIM_MODELS/LDA/decrets_urgence_2')\n",
    "wordcloud_gensim(LDA_decrets_urgence, 'NEW_GENSIM_WORDCLOUDS/LDA/decrets_urgence_2', 25, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_decrets_urgence2 = topic_modelling_NMF(decrets_urgence, 40, 'NEW_GENSIM_MODEL/nmfdecrets_urgence2')\n",
    "NMF_decrets_urgence2.save('NEW_GENSIM_MODELS/NMF/decrets_urgence2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMF_decrets_urgence2 = models.Nmf.load('NEW_GENSIM_MODELS/NMF/decrets_urgence2')\n",
    "wordcloud_gensim(NMF_decrets_urgence2, 'NEW_GENSIM_WORDCLOUDS/NMF/decrets_urgence2', 25, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESSAI CONCATENATION BULLETIN ET URGENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urgence_et_bulletin = decrets_urgence_1789_1795.append(decrets_bulletins, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urgence_et_bulletin = urgence_et_bulletin['texte'].dropna()\n",
    "liste_urgence_et_bulletin = urgence_et_bulletin.tolist()\n",
    "top2vec_urgence_et_bulletin = Top2Vec(documents=liste_urgence_et_bulletin, speed=\"deep-learn\", workers=4)\n",
    "top2vec_urgence_et_bulletin.save(\"modeles/top2vec_urgence_et_bulletin\")\n",
    "print(top2vec_urgence_et_bulletin.get_num_topics())\n",
    "wordclouds(top2vec_urgence_et_bulletin, liste_urgence_et_bulletin, 'WORDCLOUDS/urgence_et_bulletin', background_color=\"white\", colormap=\"copper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
